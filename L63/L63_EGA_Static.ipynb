{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:38:52.250651Z",
     "start_time": "2024-09-10T09:38:52.236647Z"
    }
   },
   "outputs": [],
   "source": [
    "from generate_data import generate_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torchdiffeq import odeint as odeint_torch\n",
    "from utils import reshape_dataset_to_torch, train_L63"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x241a4e50810>"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:38:52.282778Z",
     "start_time": "2024-09-10T09:38:52.262619Z"
    }
   },
   "id": "55276f33ce5eee28",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# parameters of the training experiment\n",
    "params = {'grad_mode': 'EGA-static','dim_state': 3, 'dim_output': 3, 'transition_layers': 2, 'dim_hidden_dyn_mdl': 3, 'train_size': 5000,\n",
    "          'ntrain': 600, 'dt_integration': 0.01, 'pretrained': False, 'Batch_size': 32, 'seq_size': 10,\n",
    "          'nb_part': 5, 'output_folder': 'output_models/',\n",
    "          'model_save_file_name': 'L63_EGA-static.pt', 'device': 'cuda'}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:38:52.314771Z",
     "start_time": "2024-09-10T09:38:52.294822Z"
    }
   },
   "id": "5311523c25831293",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class HybridMdl(torch.nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(HybridMdl, self).__init__()\n",
    "        # params of the true L63 system\n",
    "        self.sigma = GD.parameters.sigma\n",
    "        self.rho = GD.parameters.rho\n",
    "        self.beta = GD.parameters.beta\n",
    "\n",
    "        # params of the neural network correction\n",
    "        self.trans_layers = params['transition_layers']\n",
    "        self.transLayers = torch.nn.ModuleList([torch.nn.Linear(params['dim_state'], params['dim_hidden_dyn_mdl'])])\n",
    "        self.transLayers.extend([torch.nn.Linear(params['dim_hidden_dyn_mdl'], params['dim_hidden_dyn_mdl']) for i in\n",
    "                                 range(1, params['transition_layers'])])\n",
    "        self.out_transLayers = torch.nn.Linear(params['dim_hidden_dyn_mdl'], params['dim_state'])\n",
    "\n",
    "    def closure(self, x):\n",
    "        for i in range(self.trans_layers):\n",
    "            x = torch.tanh(self.transLayers[i](x))\n",
    "        x = self.out_transLayers(x)\n",
    "        return x\n",
    "\n",
    "    def dyn_net(self, t, inp, closure = True):\n",
    "        grad = (torch.zeros((inp.size())).to(inp.device))\n",
    "        grad[:, 0] = self.sigma * (inp[:, 1] - inp[:, 0])\n",
    "        grad[:, 1] = inp[:, 0] * (self.rho - inp[:, 2]) - inp[:, 1]\n",
    "        grad[:, 2] = inp[:, 0] * inp[:, 1]  # + self.beta*inp[:,2];\n",
    "        if closure:\n",
    "            return grad + self.closure(inp)\n",
    "        else:\n",
    "            return grad\n",
    "    def model_dt(self, inp, dt, t0=0,\n",
    "                grad_mode='exact'):  # flow of the ODE, assuming the flow is autonomous so t0 is always 0\n",
    "        if grad_mode == 'exact':\n",
    "            pred = odeint_torch(self.dyn_net, inp, torch.arange(t0, dt + 0.000001, dt).to(inp.device), method='dopri5')\n",
    "            return pred[-1, :, :]\n",
    "        elif grad_mode == 'EGA-static' or grad_mode == 'EGA-ST' or grad_mode == 'EGA-J':\n",
    "            with torch.no_grad():\n",
    "                # blackbox non diff solver, here an adaptive dopri solver\n",
    "                pred = odeint_torch(self.dyn_net, inp, torch.arange(t0, dt + 0.000001, dt).to(inp.device), method='dopri5')\n",
    "            # computational graph of the blackbox solver\n",
    "            if grad_mode == 'EGA-static':\n",
    "                output_p = dt * self.closure(inp.detach()) + inp\n",
    "                output_p.data = pred.data[-1, :, :]\n",
    "            elif grad_mode == 'EGA-ST':\n",
    "                output_p = dt * self.closure(inp) + inp\n",
    "                output_p.data = pred.data[-1, :, :]\n",
    "            elif grad_mode == 'EGA-J':\n",
    "                output_p = dt * self.dyn_net(t0+dt,inp) + inp\n",
    "                output_p.data = pred.data[-1, :, :]                \n",
    "            return output_p\n",
    "    def forward(self,dt, n, x0, closure = True, grad_mode = 'exact'):\n",
    "        pred = [x0]\n",
    "        for i in range(n):\n",
    "            pred.append(self.model_dt(pred[-1],dt, grad_mode = grad_mode))\n",
    "        pred_seq = torch.stack(pred)\n",
    "        return pred_seq"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:38:52.346684Z",
     "start_time": "2024-09-10T09:38:52.323748Z"
    }
   },
   "id": "23b8db32cff7fe20",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GD:\n",
    "    model = 'Lorenz_63'\n",
    "\n",
    "    class parameters:\n",
    "        sigma = 10.0\n",
    "        rho = 28.0\n",
    "        beta = 8.0 / 3\n",
    "\n",
    "    dt_integration = params['dt_integration']  # integration time\n",
    "    nb_loop_data = 60.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:38:52.362640Z",
     "start_time": "2024-09-10T09:38:52.349674Z"
    }
   },
   "id": "f83258809a923163",
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "342e386a0908fd59"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# run the data generation\n",
    "dataset = generate_data(GD)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:38:52.426472Z",
     "start_time": "2024-09-10T09:38:52.365632Z"
    }
   },
   "id": "f1fed4b01ffaba95",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# reshaping dataset\n",
    "X_train, Y_train, X_test, Y_test = reshape_dataset_to_torch(dataset, params['seq_size'], params['train_size'])\n",
    "training_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(training_dataset, batch_size=params['Batch_size'], shuffle=True,\n",
    "                                         pin_memory=False),\n",
    "    'val': torch.utils.data.DataLoader(val_dataset, batch_size=params['Batch_size'], shuffle=False, pin_memory=False),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:38:52.473546Z",
     "start_time": "2024-09-10T09:38:52.428467Z"
    }
   },
   "id": "84bf27ca332dd38",
   "execution_count": 28
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "hybrid_L63 = HybridMdl(params).to(params['device'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-09-10T09:38:52.505603Z",
     "start_time": "2024-09-10T09:38:52.477536Z"
    }
   },
   "id": "40e8ca0cc9bdc563",
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/599\n",
      "----------\n",
      "train: loss: 1.457373\n",
      "LR 0.01\n",
      "val: loss: 1.281035\n",
      "saving best model\n",
      "Epoch 1/599\n",
      "----------\n",
      "train: loss: 1.194116\n",
      "LR 0.01\n",
      "val: loss: 1.053099\n",
      "saving best model\n",
      "Epoch 2/599\n",
      "----------\n",
      "train: loss: 0.983219\n",
      "LR 0.01\n",
      "val: loss: 0.864311\n",
      "saving best model\n",
      "Epoch 3/599\n",
      "----------\n",
      "train: loss: 0.807185\n",
      "LR 0.01\n",
      "val: loss: 0.709399\n",
      "saving best model\n",
      "Epoch 4/599\n",
      "----------\n",
      "train: loss: 0.663337\n",
      "LR 0.01\n",
      "val: loss: 0.583341\n",
      "saving best model\n",
      "Epoch 5/599\n",
      "----------\n",
      "train: loss: 0.547040\n",
      "LR 0.01\n",
      "val: loss: 0.482830\n",
      "saving best model\n",
      "Epoch 6/599\n",
      "----------\n",
      "train: loss: 0.453174\n",
      "LR 0.01\n",
      "val: loss: 0.403763\n",
      "saving best model\n",
      "Epoch 7/599\n",
      "----------\n",
      "train: loss: 0.381806\n",
      "LR 0.01\n",
      "val: loss: 0.343034\n",
      "saving best model\n",
      "Epoch 8/599\n",
      "----------\n",
      "train: loss: 0.326411\n",
      "LR 0.01\n",
      "val: loss: 0.297228\n",
      "saving best model\n",
      "Epoch 9/599\n",
      "----------\n",
      "train: loss: 0.284008\n",
      "LR 0.01\n",
      "val: loss: 0.263769\n",
      "saving best model\n",
      "Epoch 10/599\n",
      "----------\n",
      "train: loss: 0.249448\n",
      "LR 0.01\n",
      "val: loss: 0.212239\n",
      "saving best model\n",
      "Epoch 11/599\n",
      "----------\n",
      "train: loss: 0.196239\n",
      "LR 0.01\n",
      "val: loss: 0.178648\n",
      "saving best model\n",
      "Epoch 12/599\n",
      "----------\n",
      "train: loss: 0.161680\n",
      "LR 0.01\n",
      "val: loss: 0.148740\n",
      "saving best model\n",
      "Epoch 13/599\n",
      "----------\n",
      "train: loss: 0.133945\n",
      "LR 0.01\n",
      "val: loss: 0.123203\n",
      "saving best model\n",
      "Epoch 14/599\n",
      "----------\n",
      "train: loss: 0.110701\n",
      "LR 0.01\n",
      "val: loss: 0.102742\n",
      "saving best model\n",
      "Epoch 15/599\n",
      "----------\n",
      "train: loss: 0.092602\n",
      "LR 0.01\n",
      "val: loss: 0.086370\n",
      "saving best model\n",
      "Epoch 16/599\n",
      "----------\n",
      "train: loss: 0.076943\n",
      "LR 0.01\n",
      "val: loss: 0.072108\n",
      "saving best model\n",
      "Epoch 17/599\n",
      "----------\n",
      "train: loss: 0.064109\n",
      "LR 0.01\n",
      "val: loss: 0.060112\n",
      "saving best model\n",
      "Epoch 18/599\n",
      "----------\n",
      "train: loss: 0.052808\n",
      "LR 0.01\n",
      "val: loss: 0.050296\n",
      "saving best model\n",
      "Epoch 19/599\n",
      "----------\n",
      "train: loss: 0.043753\n",
      "LR 0.01\n",
      "val: loss: 0.041898\n",
      "saving best model\n",
      "Epoch 20/599\n",
      "----------\n",
      "train: loss: 0.036534\n",
      "LR 0.01\n",
      "val: loss: 0.035036\n",
      "saving best model\n",
      "Epoch 21/599\n",
      "----------\n",
      "train: loss: 0.030506\n",
      "LR 0.01\n",
      "val: loss: 0.029192\n",
      "saving best model\n",
      "Epoch 22/599\n",
      "----------\n",
      "train: loss: 0.025084\n",
      "LR 0.01\n",
      "val: loss: 0.024742\n",
      "saving best model\n",
      "Epoch 23/599\n",
      "----------\n",
      "train: loss: 0.021045\n",
      "LR 0.01\n",
      "val: loss: 0.020265\n",
      "saving best model\n",
      "Epoch 24/599\n",
      "----------\n",
      "train: loss: 0.017456\n",
      "LR 0.01\n",
      "val: loss: 0.016874\n",
      "saving best model\n",
      "Epoch 25/599\n",
      "----------\n",
      "train: loss: 0.014443\n",
      "LR 0.01\n",
      "val: loss: 0.014156\n",
      "saving best model\n",
      "Epoch 26/599\n",
      "----------\n",
      "train: loss: 0.012215\n",
      "LR 0.01\n",
      "val: loss: 0.011966\n",
      "saving best model\n",
      "Epoch 27/599\n",
      "----------\n",
      "train: loss: 0.010284\n",
      "LR 0.01\n",
      "val: loss: 0.009840\n",
      "saving best model\n",
      "Epoch 28/599\n",
      "----------\n",
      "train: loss: 0.008651\n",
      "LR 0.01\n",
      "val: loss: 0.008339\n",
      "saving best model\n",
      "Epoch 29/599\n",
      "----------\n",
      "train: loss: 0.007280\n",
      "LR 0.01\n",
      "val: loss: 0.006890\n",
      "saving best model\n",
      "Epoch 30/599\n",
      "----------\n",
      "train: loss: 0.006105\n",
      "LR 0.01\n",
      "val: loss: 0.005826\n",
      "saving best model\n",
      "Epoch 31/599\n",
      "----------\n",
      "train: loss: 0.005204\n",
      "LR 0.01\n",
      "val: loss: 0.004878\n",
      "saving best model\n",
      "Epoch 32/599\n",
      "----------\n",
      "train: loss: 0.004530\n",
      "LR 0.01\n",
      "val: loss: 0.004256\n",
      "saving best model\n",
      "Epoch 33/599\n",
      "----------\n",
      "train: loss: 0.003881\n",
      "LR 0.01\n",
      "val: loss: 0.003583\n",
      "saving best model\n",
      "Epoch 34/599\n",
      "----------\n",
      "train: loss: 0.003416\n",
      "LR 0.01\n",
      "val: loss: 0.003175\n",
      "saving best model\n",
      "Epoch 35/599\n",
      "----------\n",
      "train: loss: 0.002876\n",
      "LR 0.01\n",
      "val: loss: 0.003372\n",
      "Epoch 36/599\n",
      "----------\n",
      "train: loss: 0.002730\n",
      "LR 0.01\n",
      "val: loss: 0.002382\n",
      "saving best model\n",
      "Epoch 37/599\n",
      "----------\n",
      "train: loss: 0.002282\n",
      "LR 0.01\n",
      "val: loss: 0.001909\n",
      "saving best model\n",
      "Epoch 38/599\n",
      "----------\n",
      "train: loss: 0.001996\n",
      "LR 0.01\n",
      "val: loss: 0.002046\n",
      "Epoch 39/599\n",
      "----------\n",
      "train: loss: 0.001734\n",
      "LR 0.01\n",
      "val: loss: 0.001325\n",
      "saving best model\n",
      "Epoch 40/599\n",
      "----------\n",
      "train: loss: 0.001518\n",
      "LR 0.01\n",
      "val: loss: 0.001166\n",
      "saving best model\n",
      "Epoch 41/599\n",
      "----------\n",
      "train: loss: 0.001375\n",
      "LR 0.01\n",
      "val: loss: 0.001034\n",
      "saving best model\n",
      "Epoch 42/599\n",
      "----------\n",
      "train: loss: 0.001264\n",
      "LR 0.01\n",
      "val: loss: 0.001039\n",
      "Epoch 43/599\n",
      "----------\n",
      "train: loss: 0.001121\n",
      "LR 0.01\n",
      "val: loss: 0.001042\n",
      "Epoch 44/599\n",
      "----------\n",
      "train: loss: 0.001036\n",
      "LR 0.01\n",
      "val: loss: 0.000660\n",
      "saving best model\n",
      "Epoch 45/599\n",
      "----------\n",
      "train: loss: 0.000877\n",
      "LR 0.01\n",
      "val: loss: 0.000643\n",
      "saving best model\n",
      "Epoch 46/599\n",
      "----------\n",
      "train: loss: 0.000870\n",
      "LR 0.01\n",
      "val: loss: 0.000668\n",
      "Epoch 47/599\n",
      "----------\n",
      "train: loss: 0.000704\n",
      "LR 0.01\n",
      "val: loss: 0.000526\n",
      "saving best model\n",
      "Epoch 48/599\n",
      "----------\n",
      "train: loss: 0.000737\n",
      "LR 0.01\n",
      "val: loss: 0.000488\n",
      "saving best model\n",
      "Epoch 49/599\n",
      "----------\n",
      "train: loss: 0.000654\n",
      "LR 0.01\n",
      "val: loss: 0.000525\n",
      "Epoch 50/599\n",
      "----------\n",
      "train: loss: 0.000555\n",
      "LR 0.01\n",
      "val: loss: 0.000417\n",
      "saving best model\n",
      "Epoch 51/599\n",
      "----------\n",
      "train: loss: 0.000492\n",
      "LR 0.01\n",
      "val: loss: 0.000403\n",
      "saving best model\n",
      "Epoch 52/599\n",
      "----------\n",
      "train: loss: 0.000495\n",
      "LR 0.01\n",
      "val: loss: 0.000387\n",
      "saving best model\n",
      "Epoch 53/599\n",
      "----------\n",
      "train: loss: 0.000475\n",
      "LR 0.01\n",
      "val: loss: 0.000155\n",
      "saving best model\n",
      "Epoch 54/599\n",
      "----------\n",
      "train: loss: 0.000407\n",
      "LR 0.01\n",
      "val: loss: 0.000161\n",
      "Epoch 55/599\n",
      "----------\n",
      "train: loss: 0.000476\n",
      "LR 0.01\n",
      "val: loss: 0.000471\n",
      "Epoch 56/599\n",
      "----------\n",
      "train: loss: 0.000377\n",
      "LR 0.01\n",
      "val: loss: 0.000283\n",
      "Epoch 57/599\n",
      "----------\n",
      "train: loss: 0.000649\n",
      "LR 0.01\n",
      "val: loss: 0.000262\n",
      "Epoch 58/599\n",
      "----------\n",
      "train: loss: 0.000389\n",
      "LR 0.01\n",
      "val: loss: 0.000267\n",
      "Epoch 59/599\n",
      "----------\n",
      "train: loss: 0.000276\n",
      "LR 0.01\n",
      "val: loss: 0.000273\n",
      "Epoch 60/599\n",
      "----------\n",
      "train: loss: 0.000268\n",
      "LR 0.01\n",
      "val: loss: 0.000116\n",
      "saving best model\n",
      "Epoch 61/599\n",
      "----------\n",
      "train: loss: 0.000273\n",
      "LR 0.01\n",
      "val: loss: 0.000622\n",
      "Epoch 62/599\n",
      "----------\n",
      "train: loss: 0.000409\n",
      "LR 0.01\n",
      "val: loss: 0.000087\n",
      "saving best model\n",
      "Epoch 63/599\n",
      "----------\n",
      "train: loss: 0.000199\n",
      "LR 0.01\n",
      "val: loss: 0.000100\n",
      "Epoch 64/599\n",
      "----------\n",
      "train: loss: 0.000171\n",
      "LR 0.01\n",
      "val: loss: 0.000070\n",
      "saving best model\n",
      "Epoch 65/599\n",
      "----------\n",
      "train: loss: 0.000161\n",
      "LR 0.01\n",
      "val: loss: 0.000153\n",
      "Epoch 66/599\n",
      "----------\n",
      "train: loss: 0.000198\n",
      "LR 0.01\n",
      "val: loss: 0.000329\n",
      "Epoch 67/599\n",
      "----------\n",
      "train: loss: 0.000235\n",
      "LR 0.01\n",
      "val: loss: 0.000147\n",
      "Epoch 68/599\n",
      "----------\n",
      "train: loss: 0.000249\n",
      "LR 0.01\n",
      "val: loss: 0.000111\n",
      "Epoch 69/599\n",
      "----------\n",
      "train: loss: 0.000198\n",
      "LR 0.01\n",
      "val: loss: 0.000129\n",
      "Epoch 70/599\n",
      "----------\n",
      "train: loss: 0.000169\n",
      "LR 0.01\n",
      "val: loss: 0.000047\n",
      "saving best model\n",
      "Epoch 71/599\n",
      "----------\n",
      "train: loss: 0.000165\n",
      "LR 0.01\n",
      "val: loss: 0.000209\n",
      "Epoch 72/599\n",
      "----------\n",
      "train: loss: 0.000184\n",
      "LR 0.01\n",
      "val: loss: 0.000604\n",
      "Epoch 73/599\n",
      "----------\n",
      "train: loss: 0.000179\n",
      "LR 0.01\n",
      "val: loss: 0.000119\n",
      "Epoch 74/599\n",
      "----------\n",
      "train: loss: 0.000132\n",
      "LR 0.01\n",
      "val: loss: 0.000196\n",
      "Epoch 75/599\n",
      "----------\n",
      "train: loss: 0.000242\n",
      "LR 0.01\n",
      "val: loss: 0.000158\n",
      "Epoch 76/599\n",
      "----------\n",
      "train: loss: 0.000139\n",
      "LR 0.01\n",
      "val: loss: 0.000049\n",
      "Epoch 77/599\n",
      "----------\n",
      "train: loss: 0.000161\n",
      "LR 0.01\n",
      "val: loss: 0.000081\n",
      "Epoch 78/599\n",
      "----------\n",
      "train: loss: 0.000224\n",
      "LR 0.01\n",
      "val: loss: 0.000310\n",
      "Epoch 79/599\n",
      "----------\n",
      "train: loss: 0.000287\n",
      "LR 0.01\n",
      "val: loss: 0.000412\n",
      "Epoch 80/599\n",
      "----------\n",
      "train: loss: 0.000328\n",
      "LR 0.01\n",
      "val: loss: 0.000053\n",
      "Epoch 81/599\n",
      "----------\n",
      "train: loss: 0.000118\n",
      "LR 0.01\n",
      "val: loss: 0.000145\n",
      "Epoch 82/599\n",
      "----------\n",
      "train: loss: 0.000113\n",
      "LR 0.01\n",
      "val: loss: 0.000164\n",
      "Epoch 83/599\n",
      "----------\n",
      "train: loss: 0.000157\n",
      "LR 0.01\n",
      "val: loss: 0.000054\n",
      "Epoch 84/599\n",
      "----------\n",
      "train: loss: 0.000173\n",
      "LR 0.01\n",
      "val: loss: 0.000050\n",
      "Epoch 85/599\n",
      "----------\n",
      "train: loss: 0.000222\n",
      "LR 0.01\n",
      "val: loss: 0.000045\n",
      "saving best model\n",
      "Epoch 86/599\n",
      "----------\n",
      "train: loss: 0.000132\n",
      "LR 0.01\n",
      "val: loss: 0.000037\n",
      "saving best model\n",
      "Epoch 87/599\n",
      "----------\n",
      "train: loss: 0.000086\n",
      "LR 0.01\n",
      "val: loss: 0.000434\n",
      "Epoch 88/599\n",
      "----------\n",
      "train: loss: 0.000238\n",
      "LR 0.01\n",
      "val: loss: 0.000182\n",
      "Epoch 89/599\n",
      "----------\n",
      "train: loss: 0.000114\n",
      "LR 0.01\n",
      "val: loss: 0.000127\n",
      "Epoch 90/599\n",
      "----------\n",
      "train: loss: 0.000452\n",
      "LR 0.01\n",
      "val: loss: 0.000229\n",
      "Epoch 91/599\n",
      "----------\n",
      "train: loss: 0.000091\n",
      "LR 0.01\n",
      "val: loss: 0.000288\n",
      "Epoch 92/599\n",
      "----------\n",
      "train: loss: 0.000094\n",
      "LR 0.01\n",
      "val: loss: 0.000072\n",
      "Epoch 93/599\n",
      "----------\n",
      "train: loss: 0.000075\n",
      "LR 0.01\n",
      "val: loss: 0.000067\n",
      "Epoch 94/599\n",
      "----------\n",
      "train: loss: 0.000756\n",
      "LR 0.01\n",
      "val: loss: 0.005125\n",
      "Epoch 95/599\n",
      "----------\n",
      "train: loss: 0.000616\n",
      "LR 0.01\n",
      "val: loss: 0.000067\n",
      "Epoch 96/599\n",
      "----------\n",
      "train: loss: 0.000080\n",
      "LR 0.01\n",
      "val: loss: 0.000021\n",
      "saving best model\n",
      "Epoch 97/599\n",
      "----------\n",
      "train: loss: 0.000074\n",
      "LR 0.01\n",
      "val: loss: 0.000066\n",
      "Epoch 98/599\n",
      "----------\n",
      "train: loss: 0.000071\n",
      "LR 0.01\n",
      "val: loss: 0.000146\n",
      "Epoch 99/599\n",
      "----------\n",
      "train: loss: 0.000146\n",
      "LR 0.001\n",
      "val: loss: 0.000073\n",
      "Epoch 100/599\n",
      "----------\n",
      "train: loss: 0.000037\n",
      "LR 0.001\n",
      "val: loss: 0.000017\n",
      "saving best model\n",
      "Epoch 101/599\n",
      "----------\n",
      "train: loss: 0.000034\n",
      "LR 0.001\n",
      "val: loss: 0.000016\n",
      "saving best model\n",
      "Epoch 102/599\n",
      "----------\n",
      "train: loss: 0.000034\n",
      "LR 0.001\n",
      "val: loss: 0.000020\n",
      "Epoch 103/599\n",
      "----------\n",
      "train: loss: 0.000034\n",
      "LR 0.001\n",
      "val: loss: 0.000018\n",
      "Epoch 104/599\n",
      "----------\n",
      "train: loss: 0.000034\n",
      "LR 0.001\n",
      "val: loss: 0.000024\n",
      "Epoch 105/599\n",
      "----------\n",
      "train: loss: 0.000034\n",
      "LR 0.001\n",
      "val: loss: 0.000017\n",
      "Epoch 106/599\n",
      "----------\n",
      "train: loss: 0.000034\n",
      "LR 0.001\n",
      "val: loss: 0.000026\n",
      "Epoch 107/599\n",
      "----------\n",
      "train: loss: 0.000034\n",
      "LR 0.001\n",
      "val: loss: 0.000023\n",
      "Epoch 108/599\n",
      "----------\n",
      "train: loss: 0.000033\n",
      "LR 0.001\n",
      "val: loss: 0.000026\n",
      "Epoch 109/599\n",
      "----------\n",
      "train: loss: 0.000033\n",
      "LR 0.001\n",
      "val: loss: 0.000019\n",
      "Epoch 110/599\n",
      "----------\n",
      "train: loss: 0.000033\n",
      "LR 0.001\n",
      "val: loss: 0.000016\n",
      "saving best model\n",
      "Epoch 111/599\n",
      "----------\n",
      "train: loss: 0.000033\n",
      "LR 0.001\n",
      "val: loss: 0.000017\n",
      "Epoch 112/599\n",
      "----------\n",
      "train: loss: 0.000033\n",
      "LR 0.001\n",
      "val: loss: 0.000022\n",
      "Epoch 113/599\n",
      "----------\n",
      "train: loss: 0.000034\n",
      "LR 0.001\n",
      "val: loss: 0.000025\n",
      "Epoch 114/599\n",
      "----------\n",
      "train: loss: 0.000033\n",
      "LR 0.001\n",
      "val: loss: 0.000023\n",
      "Epoch 115/599\n",
      "----------\n",
      "train: loss: 0.000033\n",
      "LR 0.001\n",
      "val: loss: 0.000015\n",
      "saving best model\n",
      "Epoch 116/599\n",
      "----------\n",
      "train: loss: 0.000032\n",
      "LR 0.001\n",
      "val: loss: 0.000016\n",
      "Epoch 117/599\n",
      "----------\n",
      "train: loss: 0.000033\n",
      "LR 0.001\n",
      "val: loss: 0.000029\n",
      "Epoch 118/599\n",
      "----------\n",
      "train: loss: 0.000033\n",
      "LR 0.001\n",
      "val: loss: 0.000013\n",
      "saving best model\n",
      "Epoch 119/599\n",
      "----------\n",
      "train: loss: 0.000031\n",
      "LR 0.001\n",
      "val: loss: 0.000023\n",
      "Epoch 120/599\n",
      "----------\n",
      "train: loss: 0.000031\n",
      "LR 0.001\n",
      "val: loss: 0.000017\n",
      "Epoch 121/599\n",
      "----------\n",
      "train: loss: 0.000030\n",
      "LR 0.001\n",
      "val: loss: 0.000019\n",
      "Epoch 122/599\n",
      "----------\n",
      "train: loss: 0.000032\n",
      "LR 0.001\n",
      "val: loss: 0.000019\n",
      "Epoch 123/599\n",
      "----------\n",
      "train: loss: 0.000030\n",
      "LR 0.001\n",
      "val: loss: 0.000032\n",
      "Epoch 124/599\n",
      "----------\n",
      "train: loss: 0.000029\n",
      "LR 0.001\n",
      "val: loss: 0.000019\n",
      "Epoch 125/599\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "optimizer_ft = torch.optim.Adam(hybrid_L63.parameters(), lr=0.01)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=100, gamma=0.1)\n",
    "model_best_valid, model, loss_train, loss_val = train_L63(hybrid_L63, dataloaders, optimizer_ft, exp_lr_scheduler, device=params['device'], num_epochs=params['ntrain'], dt=params['dt_integration'], seq_size=params['seq_size'], grad_mode=params['grad_mode'])\n",
    "torch.save(model.state_dict(), params['output_folder'] + params['model_save_file_name'])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-09-10T09:38:52.508594Z"
    }
   },
   "id": "600d820290ff1337",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "initial_condition_test = next(iter(dataloaders['val']))[0][:1,:]\n",
    "with torch.no_grad():\n",
    "    simulation_hybrid = hybrid_L63(0.01, 1000, initial_condition_test.to(params['device']))[:,0,:]\n",
    "    simulation_init_sys = odeint_torch(\n",
    "        lambda t, inp: hybrid_L63.dyn_net(t, inp, closure=False), \n",
    "        initial_condition_test.to(params['device']), \n",
    "        torch.arange(0, 10 + 0.000001, 0.01).to(params['device']), \n",
    "        method='dopri5'\n",
    "    )[:,0,:]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6e2363b31d5f112f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plotting the line in 3D\n",
    "ax.plot(dataset[:,0], dataset[:,1], dataset[:,2], label='True Attractor', alpha = 0.3)\n",
    "ax.plot(simulation_hybrid.cpu().numpy()[:,0], simulation_hybrid.cpu().numpy()[:,1], simulation_hybrid.cpu().numpy()[:,2], label='Hybrid model, EGA Static ')\n",
    "ax.plot(simulation_init_sys.cpu().numpy()[:,0], simulation_init_sys.cpu().numpy()[:,1], simulation_init_sys.cpu().numpy()[:,2], label='Physical core')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a5a3d8934ee3ca2f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "531a8dc588dac8d0",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
