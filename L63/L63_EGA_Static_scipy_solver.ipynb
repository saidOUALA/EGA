{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "The Lorenz 63 dynamical system is a 3-dimensional model of the form:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    \\dot{{u}}^{\\dagger}_{t,1} &= \\sigma({u}^{\\dagger}_{t,2}-{u}^{\\dagger}_{t,1})\\\\\n",
    "    \\dot{{u}}^{\\dagger}_{t,2} &= \\rho {u}^{\\dagger}_{t,1} - {u}^{\\dagger}_{t,2} - {u}^{\\dagger}_{t,1}{u}^{\\dagger}_{t,3}\\\\\n",
    "    \\dot{{u}}^{\\dagger}_{t,3} &= {u}^{\\dagger}_{t,1}{u}^{\\dagger}_{t,2} - \\beta{u}^{\\dagger}_{t,3}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "Under parametrization $\\sigma =10$, $\\rho=28$ and  $\\beta=8/3$, this system exhibits chaotic dynamics with a strange attractor.\n",
    "\n",
    "We assume here that we are provided with $\\mathrm{F}$, an imperfect version of the above Lorenz system that does not include the term $\\beta{u}^{\\dagger}_{t,3}$. This new model can not simulate the Lorenz 63 strange attractor and converges to an equilibrium point. We correct this physical core with a sub-model $\\mathrm{M}_{\\mathrm{\\theta}}$ as follows:\n",
    "\\begin{equation}\n",
    "\\dot{\\mathrm{u}}_t = \\underbrace{\\mathrm{F}(\\mathrm{u}_t) + \\mathrm{M}_{\\mathrm{\\theta}}(\\mathrm{u}_t)}_{\\text{Hybrid model}}\n",
    "\\end{equation}\n",
    "where $\\mathrm{u}_t = [{u}_{t,1},{u}_{t,2},{u}_{t,3}]^T$ and $\\mathrm{F}: \\mathbb{R}^3 \\longrightarrow \\mathbb{R}^3$ is given by:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "    F_1(\\mathrm{u}_t) &= \\sigma({u}_{t,2}-{u}_{t,1})\\\\\n",
    "    F_2(\\mathrm{u}_t) &= \\rho {u}_{t,1} - {u}_{t,2} - {u}_{t,1}{u}_{t,3}\\\\\n",
    "    F_3(\\mathrm{u}_t) &= {u}_{t,1}{u}_{t,2}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "The sub-model $\\mathrm{M}_{\\mathrm{\\theta}}$ is a fully connected neural network with parameters $\\mathrm{\\theta}$. In this notebook, we show how to use the Static EGA online learning method when the forward solver is the scipy ODEINT solver, which does not embed automatic differentiation."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61556ecdfe29046f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from generate_data import generate_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from scipy.integrate import odeint\n",
    "from utils import reshape_dataset_to_torch, train_L63"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T16:34:06.161058Z",
     "start_time": "2024-10-23T16:34:02.350144Z"
    }
   },
   "id": "initial_id",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x29305e81790>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T16:34:06.192161Z",
     "start_time": "2024-10-23T16:34:06.165259Z"
    }
   },
   "id": "55276f33ce5eee28",
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# parameters of the training experiment\n",
    "params = {'grad_mode': 'EGA-static','dim_state': 3, 'dim_output': 3, 'transition_layers': 2, 'dim_hidden_dyn_mdl': 3, 'train_size': 5000,\n",
    "          'ntrain': 600, 'dt_integration': 0.01, 'pretrained': False, 'Batch_size': 32, 'seq_size': 10,\n",
    "          'nb_part': 5, 'output_folder': 'output_models/',\n",
    "          'model_save_file_name': 'L63_EGA-static.pt', 'device': 'cuda'}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T16:34:06.207674Z",
     "start_time": "2024-10-23T16:34:06.194215Z"
    }
   },
   "id": "5311523c25831293",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class HybridMdl(torch.nn.Module):\n",
    "    def __init__(self, params):\n",
    "        super(HybridMdl, self).__init__()\n",
    "        # params of the true L63 system\n",
    "        self.device = params['device']\n",
    "        self.sigma = GD.parameters.sigma\n",
    "        self.rho = GD.parameters.rho\n",
    "        self.beta = GD.parameters.beta\n",
    "\n",
    "        # params of the neural network correction\n",
    "        self.trans_layers = params['transition_layers']\n",
    "        self.transLayers = torch.nn.ModuleList([torch.nn.Linear(params['dim_state'], params['dim_hidden_dyn_mdl'])])\n",
    "        self.transLayers.extend([torch.nn.Linear(params['dim_hidden_dyn_mdl'], params['dim_hidden_dyn_mdl']) for i in\n",
    "                                 range(1, params['transition_layers'])])\n",
    "        self.out_transLayers = torch.nn.Linear(params['dim_hidden_dyn_mdl'], params['dim_state'])\n",
    "\n",
    "    def closure(self, x):\n",
    "        for i in range(self.trans_layers):\n",
    "            x = torch.tanh(self.transLayers[i](x))\n",
    "        x = self.out_transLayers(x)\n",
    "        return x\n",
    "\n",
    "    def dyn_net(self, inp, t, closure = True):\n",
    "        grad = np.zeros((inp.shape[0]))\n",
    "        grad[0] = self.sigma * (inp[1] - inp[0])\n",
    "        grad[1] = inp[0] * (self.rho - inp[2]) - inp[1]\n",
    "        grad[2] = inp[0] * inp[1]  # + self.beta*inp[:,2];\n",
    "        if closure:\n",
    "            return grad + self.closure(torch.from_numpy(inp[np.newaxis]).float().to(self.device)).cpu().numpy()[0,:]\n",
    "        else:\n",
    "            return grad\n",
    "    def model_dt(self, inp, dt, t0=0,\n",
    "                grad_mode='exact'):  # flow of the ODE, assuming the flow is autonomous so t0 is always 0\n",
    "        with torch.no_grad():\n",
    "            pred = torch.zeros_like(inp)\n",
    "            for i in range(inp.shape[0]):\n",
    "                # blackbox non diff solver\n",
    "                pred[i,:] = torch.from_numpy(odeint(self.dyn_net, inp[i,:].cpu().numpy(), np.arange(t0, dt + 0.000001, dt))[-1,:]).float().to(inp.device)\n",
    "        # computational graph of the blackbox solver\n",
    "        if grad_mode == 'EGA-static':\n",
    "            output_p = dt * self.closure(inp.detach()) + inp\n",
    "            output_p.data = pred.data[:, :]\n",
    "        elif grad_mode == 'EGA-ST':\n",
    "            output_p = dt * self.closure(inp) + inp\n",
    "            output_p.data = pred.data[:, :]\n",
    "        elif grad_mode == 'EGA-J':\n",
    "            output_p = dt * self.dyn_net(inp, t0+dt) + inp\n",
    "            output_p.data = pred.data[:, :]                \n",
    "        return output_p\n",
    "    def forward(self,dt, n, x0, closure = True, grad_mode = 'EGA-static'):\n",
    "        pred = [x0]\n",
    "        for i in range(n):\n",
    "            pred.append(self.model_dt(pred[-1],dt, grad_mode = grad_mode))\n",
    "        pred_seq = torch.stack(pred)\n",
    "        return pred_seq"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T16:34:06.238767Z",
     "start_time": "2024-10-23T16:34:06.209822Z"
    }
   },
   "id": "23b8db32cff7fe20",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class GD:\n",
    "    model = 'Lorenz_63'\n",
    "\n",
    "    class parameters:\n",
    "        sigma = 10.0\n",
    "        rho = 28.0\n",
    "        beta = 8.0 / 3\n",
    "\n",
    "    dt_integration = params['dt_integration']  # integration time\n",
    "    nb_loop_data = 60.0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T16:34:06.254223Z",
     "start_time": "2024-10-23T16:34:06.244985Z"
    }
   },
   "id": "f83258809a923163",
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "342e386a0908fd59"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# run the data generation\n",
    "dataset = generate_data(GD)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T16:34:06.316491Z",
     "start_time": "2024-10-23T16:34:06.256349Z"
    }
   },
   "id": "f1fed4b01ffaba95",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# reshaping dataset\n",
    "X_train, Y_train, X_test, Y_test = reshape_dataset_to_torch(dataset, params['seq_size'], params['train_size'])\n",
    "training_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "val_dataset = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "dataloaders = {\n",
    "    'train': torch.utils.data.DataLoader(training_dataset, batch_size=params['Batch_size'], shuffle=True,\n",
    "                                         pin_memory=False),\n",
    "    'val': torch.utils.data.DataLoader(val_dataset, batch_size=params['Batch_size'], shuffle=False, pin_memory=False),\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T16:34:06.363683Z",
     "start_time": "2024-10-23T16:34:06.318565Z"
    }
   },
   "id": "84bf27ca332dd38",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "hybrid_L63 = HybridMdl(params).to(params['device'])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-10-23T16:34:06.514260Z",
     "start_time": "2024-10-23T16:34:06.366851Z"
    }
   },
   "id": "40e8ca0cc9bdc563",
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/599\n",
      "----------\n",
      "train: loss: 1.457372\n",
      "LR 0.01\n",
      "val: loss: 1.281035\n",
      "saving best model\n",
      "Epoch 1/599\n",
      "----------\n",
      "train: loss: 1.194115\n",
      "LR 0.01\n",
      "val: loss: 1.053097\n",
      "saving best model\n",
      "Epoch 2/599\n",
      "----------\n",
      "train: loss: 0.983219\n",
      "LR 0.01\n",
      "val: loss: 0.864311\n",
      "saving best model\n",
      "Epoch 3/599\n",
      "----------\n",
      "train: loss: 0.807184\n",
      "LR 0.01\n",
      "val: loss: 0.709399\n",
      "saving best model\n",
      "Epoch 4/599\n",
      "----------\n",
      "train: loss: 0.663336\n",
      "LR 0.01\n",
      "val: loss: 0.583342\n",
      "saving best model\n",
      "Epoch 5/599\n",
      "----------\n",
      "train: loss: 0.547040\n",
      "LR 0.01\n",
      "val: loss: 0.482830\n",
      "saving best model\n",
      "Epoch 6/599\n",
      "----------\n",
      "train: loss: 0.453174\n",
      "LR 0.01\n",
      "val: loss: 0.403762\n",
      "saving best model\n",
      "Epoch 7/599\n",
      "----------\n",
      "train: loss: 0.381806\n",
      "LR 0.01\n",
      "val: loss: 0.343034\n",
      "saving best model\n",
      "Epoch 8/599\n",
      "----------\n",
      "train: loss: 0.326411\n",
      "LR 0.01\n",
      "val: loss: 0.297226\n",
      "saving best model\n",
      "Epoch 9/599\n",
      "----------\n",
      "train: loss: 0.284008\n",
      "LR 0.01\n",
      "val: loss: 0.263769\n",
      "saving best model\n",
      "Epoch 10/599\n",
      "----------\n",
      "train: loss: 0.249405\n",
      "LR 0.01\n",
      "val: loss: 0.212902\n",
      "saving best model\n",
      "Epoch 11/599\n",
      "----------\n",
      "train: loss: 0.196078\n",
      "LR 0.01\n",
      "val: loss: 0.178445\n",
      "saving best model\n",
      "Epoch 12/599\n",
      "----------\n",
      "train: loss: 0.161659\n",
      "LR 0.01\n",
      "val: loss: 0.148693\n",
      "saving best model\n",
      "Epoch 13/599\n",
      "----------\n",
      "train: loss: 0.133930\n",
      "LR 0.01\n",
      "val: loss: 0.123173\n",
      "saving best model\n",
      "Epoch 14/599\n",
      "----------\n",
      "train: loss: 0.110673\n",
      "LR 0.01\n",
      "val: loss: 0.102727\n",
      "saving best model\n",
      "Epoch 15/599\n",
      "----------\n",
      "train: loss: 0.092586\n",
      "LR 0.01\n",
      "val: loss: 0.086362\n",
      "saving best model\n",
      "Epoch 16/599\n",
      "----------\n",
      "train: loss: 0.076932\n",
      "LR 0.01\n",
      "val: loss: 0.072121\n",
      "saving best model\n",
      "Epoch 17/599\n",
      "----------\n",
      "train: loss: 0.064096\n",
      "LR 0.01\n",
      "val: loss: 0.060096\n",
      "saving best model\n",
      "Epoch 18/599\n",
      "----------\n",
      "train: loss: 0.052799\n",
      "LR 0.01\n",
      "val: loss: 0.050278\n",
      "saving best model\n",
      "Epoch 19/599\n",
      "----------\n",
      "train: loss: 0.043745\n",
      "LR 0.01\n",
      "val: loss: 0.041892\n",
      "saving best model\n",
      "Epoch 20/599\n",
      "----------\n",
      "train: loss: 0.036525\n",
      "LR 0.01\n",
      "val: loss: 0.035027\n",
      "saving best model\n",
      "Epoch 21/599\n",
      "----------\n",
      "train: loss: 0.030500\n",
      "LR 0.01\n",
      "val: loss: 0.029179\n",
      "saving best model\n",
      "Epoch 22/599\n",
      "----------\n",
      "train: loss: 0.025078\n",
      "LR 0.01\n",
      "val: loss: 0.024734\n",
      "saving best model\n",
      "Epoch 23/599\n",
      "----------\n",
      "train: loss: 0.021038\n",
      "LR 0.01\n",
      "val: loss: 0.020259\n",
      "saving best model\n",
      "Epoch 24/599\n",
      "----------\n",
      "train: loss: 0.017451\n",
      "LR 0.01\n",
      "val: loss: 0.016871\n",
      "saving best model\n",
      "Epoch 25/599\n",
      "----------\n",
      "train: loss: 0.014439\n",
      "LR 0.01\n",
      "val: loss: 0.014151\n",
      "saving best model\n",
      "Epoch 26/599\n",
      "----------\n",
      "train: loss: 0.012211\n",
      "LR 0.01\n",
      "val: loss: 0.011957\n",
      "saving best model\n",
      "Epoch 27/599\n",
      "----------\n",
      "train: loss: 0.010278\n",
      "LR 0.01\n",
      "val: loss: 0.009833\n",
      "saving best model\n",
      "Epoch 28/599\n",
      "----------\n",
      "train: loss: 0.008649\n",
      "LR 0.01\n",
      "val: loss: 0.008334\n",
      "saving best model\n",
      "Epoch 29/599\n",
      "----------\n",
      "train: loss: 0.007277\n",
      "LR 0.01\n",
      "val: loss: 0.006885\n",
      "saving best model\n",
      "Epoch 30/599\n",
      "----------\n",
      "train: loss: 0.006101\n",
      "LR 0.01\n",
      "val: loss: 0.005821\n",
      "saving best model\n",
      "Epoch 31/599\n",
      "----------\n",
      "train: loss: 0.005201\n",
      "LR 0.01\n",
      "val: loss: 0.004874\n",
      "saving best model\n",
      "Epoch 32/599\n",
      "----------\n",
      "train: loss: 0.004526\n",
      "LR 0.01\n",
      "val: loss: 0.004261\n",
      "saving best model\n",
      "Epoch 33/599\n",
      "----------\n",
      "train: loss: 0.003878\n",
      "LR 0.01\n",
      "val: loss: 0.003571\n",
      "saving best model\n",
      "Epoch 34/599\n",
      "----------\n",
      "train: loss: 0.003414\n",
      "LR 0.01\n",
      "val: loss: 0.003177\n",
      "saving best model\n",
      "Epoch 35/599\n",
      "----------\n",
      "train: loss: 0.002875\n",
      "LR 0.01\n",
      "val: loss: 0.003357\n",
      "Epoch 36/599\n",
      "----------\n",
      "train: loss: 0.002717\n",
      "LR 0.01\n",
      "val: loss: 0.002374\n",
      "saving best model\n",
      "Epoch 37/599\n",
      "----------\n",
      "train: loss: 0.002281\n",
      "LR 0.01\n",
      "val: loss: 0.001909\n",
      "saving best model\n",
      "Epoch 38/599\n",
      "----------\n",
      "train: loss: 0.001995\n",
      "LR 0.01\n",
      "val: loss: 0.002057\n",
      "Epoch 39/599\n",
      "----------\n",
      "train: loss: 0.001728\n",
      "LR 0.01\n",
      "val: loss: 0.001332\n",
      "saving best model\n",
      "Epoch 40/599\n",
      "----------\n",
      "train: loss: 0.001513\n",
      "LR 0.01\n",
      "val: loss: 0.001161\n",
      "saving best model\n",
      "Epoch 41/599\n",
      "----------\n",
      "train: loss: 0.001367\n",
      "LR 0.01\n",
      "val: loss: 0.001014\n",
      "saving best model\n",
      "Epoch 42/599\n",
      "----------\n",
      "train: loss: 0.001268\n",
      "LR 0.01\n",
      "val: loss: 0.001086\n",
      "Epoch 43/599\n",
      "----------\n",
      "train: loss: 0.001118\n",
      "LR 0.01\n",
      "val: loss: 0.000940\n",
      "saving best model\n",
      "Epoch 44/599\n",
      "----------\n",
      "train: loss: 0.001038\n",
      "LR 0.01\n",
      "val: loss: 0.000657\n",
      "saving best model\n",
      "Epoch 45/599\n",
      "----------\n",
      "train: loss: 0.000866\n",
      "LR 0.01\n",
      "val: loss: 0.000637\n",
      "saving best model\n",
      "Epoch 46/599\n",
      "----------\n",
      "train: loss: 0.000854\n",
      "LR 0.01\n",
      "val: loss: 0.000602\n",
      "saving best model\n",
      "Epoch 47/599\n",
      "----------\n",
      "train: loss: 0.000714\n",
      "LR 0.01\n",
      "val: loss: 0.000537\n",
      "saving best model\n",
      "Epoch 48/599\n",
      "----------\n",
      "train: loss: 0.000741\n",
      "LR 0.01\n",
      "val: loss: 0.000512\n",
      "saving best model\n",
      "Epoch 49/599\n",
      "----------\n",
      "train: loss: 0.000660\n",
      "LR 0.01\n",
      "val: loss: 0.000530\n",
      "Epoch 50/599\n",
      "----------\n",
      "train: loss: 0.000552\n",
      "LR 0.01\n",
      "val: loss: 0.000415\n",
      "saving best model\n",
      "Epoch 51/599\n",
      "----------\n",
      "train: loss: 0.000487\n",
      "LR 0.01\n",
      "val: loss: 0.000423\n",
      "Epoch 52/599\n",
      "----------\n",
      "train: loss: 0.000489\n",
      "LR 0.01\n",
      "val: loss: 0.000387\n",
      "saving best model\n",
      "Epoch 53/599\n",
      "----------\n",
      "train: loss: 0.000475\n",
      "LR 0.01\n",
      "val: loss: 0.000153\n",
      "saving best model\n",
      "Epoch 54/599\n",
      "----------\n",
      "train: loss: 0.000400\n",
      "LR 0.01\n",
      "val: loss: 0.000168\n",
      "Epoch 55/599\n",
      "----------\n",
      "train: loss: 0.000466\n",
      "LR 0.01\n",
      "val: loss: 0.000476\n",
      "Epoch 56/599\n",
      "----------\n",
      "train: loss: 0.000363\n",
      "LR 0.01\n",
      "val: loss: 0.000233\n",
      "Epoch 57/599\n",
      "----------\n",
      "train: loss: 0.000535\n",
      "LR 0.01\n",
      "val: loss: 0.000390\n",
      "Epoch 58/599\n",
      "----------\n",
      "train: loss: 0.000426\n",
      "LR 0.01\n",
      "val: loss: 0.000300\n",
      "Epoch 59/599\n",
      "----------\n",
      "train: loss: 0.000285\n",
      "LR 0.01\n",
      "val: loss: 0.000243\n",
      "Epoch 60/599\n",
      "----------\n",
      "train: loss: 0.000280\n",
      "LR 0.01\n",
      "val: loss: 0.000139\n",
      "saving best model\n",
      "Epoch 61/599\n",
      "----------\n",
      "train: loss: 0.000269\n",
      "LR 0.01\n",
      "val: loss: 0.000697\n",
      "Epoch 62/599\n",
      "----------\n",
      "train: loss: 0.000434\n",
      "LR 0.01\n",
      "val: loss: 0.000096\n",
      "saving best model\n",
      "Epoch 63/599\n",
      "----------\n",
      "train: loss: 0.000199\n",
      "LR 0.01\n",
      "val: loss: 0.000111\n",
      "Epoch 64/599\n",
      "----------\n",
      "train: loss: 0.000185\n",
      "LR 0.01\n",
      "val: loss: 0.000067\n",
      "saving best model\n",
      "Epoch 65/599\n",
      "----------\n",
      "train: loss: 0.000157\n",
      "LR 0.01\n",
      "val: loss: 0.000162\n",
      "Epoch 66/599\n",
      "----------\n",
      "train: loss: 0.000191\n",
      "LR 0.01\n",
      "val: loss: 0.000285\n",
      "Epoch 67/599\n",
      "----------\n",
      "train: loss: 0.000228\n",
      "LR 0.01\n",
      "val: loss: 0.000180\n",
      "Epoch 68/599\n",
      "----------\n",
      "train: loss: 0.000192\n",
      "LR 0.01\n",
      "val: loss: 0.000250\n",
      "Epoch 69/599\n",
      "----------\n",
      "train: loss: 0.000194\n",
      "LR 0.01\n",
      "val: loss: 0.000148\n",
      "Epoch 70/599\n",
      "----------\n",
      "train: loss: 0.000171\n",
      "LR 0.01\n",
      "val: loss: 0.000046\n",
      "saving best model\n",
      "Epoch 71/599\n",
      "----------\n",
      "train: loss: 0.000176\n",
      "LR 0.01\n",
      "val: loss: 0.000224\n",
      "Epoch 72/599\n",
      "----------\n",
      "train: loss: 0.000211\n",
      "LR 0.01\n",
      "val: loss: 0.000697\n",
      "Epoch 73/599\n",
      "----------\n",
      "train: loss: 0.000158\n",
      "LR 0.01\n",
      "val: loss: 0.000085\n",
      "Epoch 74/599\n",
      "----------\n",
      "train: loss: 0.000154\n",
      "LR 0.01\n",
      "val: loss: 0.000236\n",
      "Epoch 75/599\n",
      "----------\n",
      "train: loss: 0.000231\n",
      "LR 0.01\n",
      "val: loss: 0.000134\n",
      "Epoch 76/599\n",
      "----------\n",
      "train: loss: 0.000131\n",
      "LR 0.01\n",
      "val: loss: 0.000042\n",
      "saving best model\n",
      "Epoch 77/599\n",
      "----------\n",
      "train: loss: 0.000157\n",
      "LR 0.01\n",
      "val: loss: 0.000045\n",
      "Epoch 78/599\n",
      "----------\n",
      "train: loss: 0.000210\n",
      "LR 0.01\n",
      "val: loss: 0.000218\n",
      "Epoch 79/599\n",
      "----------\n",
      "train: loss: 0.000348\n",
      "LR 0.01\n",
      "val: loss: 0.000430\n",
      "Epoch 80/599\n",
      "----------\n",
      "train: loss: 0.000344\n",
      "LR 0.01\n",
      "val: loss: 0.000050\n",
      "Epoch 81/599\n",
      "----------\n",
      "train: loss: 0.000112\n",
      "LR 0.01\n",
      "val: loss: 0.000064\n",
      "Epoch 82/599\n",
      "----------\n",
      "train: loss: 0.000101\n",
      "LR 0.01\n",
      "val: loss: 0.000161\n",
      "Epoch 83/599\n",
      "----------\n",
      "train: loss: 0.000148\n",
      "LR 0.01\n",
      "val: loss: 0.000037\n",
      "saving best model\n",
      "Epoch 84/599\n",
      "----------\n",
      "train: loss: 0.000102\n",
      "LR 0.01\n",
      "val: loss: 0.000061\n",
      "Epoch 85/599\n",
      "----------\n",
      "train: loss: 0.000224\n",
      "LR 0.01\n",
      "val: loss: 0.000058\n",
      "Epoch 86/599\n",
      "----------\n",
      "train: loss: 0.000164\n",
      "LR 0.01\n",
      "val: loss: 0.000048\n",
      "Epoch 87/599\n",
      "----------\n",
      "train: loss: 0.000084\n",
      "LR 0.01\n",
      "val: loss: 0.000309\n",
      "Epoch 88/599\n",
      "----------\n",
      "train: loss: 0.000246\n",
      "LR 0.01\n",
      "val: loss: 0.000154\n",
      "Epoch 89/599\n",
      "----------\n",
      "train: loss: 0.000131\n",
      "LR 0.01\n",
      "val: loss: 0.000163\n",
      "Epoch 90/599\n",
      "----------\n",
      "train: loss: 0.000376\n",
      "LR 0.01\n",
      "val: loss: 0.000051\n",
      "Epoch 91/599\n",
      "----------\n",
      "train: loss: 0.000106\n",
      "LR 0.01\n",
      "val: loss: 0.000441\n",
      "Epoch 92/599\n",
      "----------\n",
      "train: loss: 0.000131\n",
      "LR 0.01\n",
      "val: loss: 0.000111\n",
      "Epoch 93/599\n",
      "----------\n",
      "train: loss: 0.000084\n",
      "LR 0.01\n",
      "val: loss: 0.000073\n",
      "Epoch 94/599\n",
      "----------\n",
      "train: loss: 0.000787\n",
      "LR 0.01\n",
      "val: loss: 0.006593\n",
      "Epoch 95/599\n",
      "----------\n",
      "train: loss: 0.000517\n",
      "LR 0.01\n",
      "val: loss: 0.000061\n",
      "Epoch 96/599\n",
      "----------\n",
      "train: loss: 0.000088\n",
      "LR 0.01\n",
      "val: loss: 0.000020\n",
      "saving best model\n",
      "Epoch 97/599\n",
      "----------\n",
      "train: loss: 0.000083\n",
      "LR 0.01\n",
      "val: loss: 0.000122\n",
      "Epoch 98/599\n",
      "----------\n",
      "train: loss: 0.000089\n",
      "LR 0.01\n",
      "val: loss: 0.000189\n",
      "Epoch 99/599\n",
      "----------\n",
      "train: loss: 0.000177\n",
      "LR 0.001\n",
      "val: loss: 0.000091\n",
      "Epoch 100/599\n",
      "----------\n",
      "train: loss: 0.000029\n",
      "LR 0.001\n",
      "val: loss: 0.000015\n",
      "saving best model\n",
      "Epoch 101/599\n",
      "----------\n",
      "train: loss: 0.000027\n",
      "LR 0.001\n",
      "val: loss: 0.000015\n",
      "saving best model\n",
      "Epoch 102/599\n",
      "----------\n",
      "train: loss: 0.000026\n",
      "LR 0.001\n",
      "val: loss: 0.000017\n",
      "Epoch 103/599\n",
      "----------\n",
      "train: loss: 0.000026\n",
      "LR 0.001\n",
      "val: loss: 0.000013\n",
      "saving best model\n",
      "Epoch 104/599\n",
      "----------\n",
      "train: loss: 0.000026\n",
      "LR 0.001\n",
      "val: loss: 0.000017\n",
      "Epoch 105/599\n",
      "----------\n",
      "train: loss: 0.000026\n",
      "LR 0.001\n",
      "val: loss: 0.000014\n",
      "Epoch 106/599\n",
      "----------\n",
      "train: loss: 0.000026\n",
      "LR 0.001\n",
      "val: loss: 0.000019\n",
      "Epoch 107/599\n",
      "----------\n",
      "train: loss: 0.000027\n",
      "LR 0.001\n",
      "val: loss: 0.000015\n",
      "Epoch 108/599\n",
      "----------\n",
      "train: loss: 0.000026\n",
      "LR 0.001\n",
      "val: loss: 0.000022\n",
      "Epoch 109/599\n",
      "----------\n",
      "train: loss: 0.000026\n",
      "LR 0.001\n",
      "val: loss: 0.000016\n",
      "Epoch 110/599\n",
      "----------\n",
      "train: loss: 0.000025\n",
      "LR 0.001\n",
      "val: loss: 0.000012\n",
      "saving best model\n",
      "Epoch 111/599\n",
      "----------\n",
      "train: loss: 0.000025\n",
      "LR 0.001\n",
      "val: loss: 0.000012\n",
      "Epoch 112/599\n",
      "----------\n",
      "train: loss: 0.000025\n",
      "LR 0.001\n",
      "val: loss: 0.000018\n",
      "Epoch 113/599\n",
      "----------\n",
      "train: loss: 0.000027\n",
      "LR 0.001\n",
      "val: loss: 0.000023\n",
      "Epoch 114/599\n",
      "----------\n",
      "train: loss: 0.000026\n",
      "LR 0.001\n",
      "val: loss: 0.000018\n",
      "Epoch 115/599\n",
      "----------\n",
      "train: loss: 0.000026\n",
      "LR 0.001\n",
      "val: loss: 0.000013\n",
      "Epoch 116/599\n",
      "----------\n",
      "train: loss: 0.000025\n",
      "LR 0.001\n",
      "val: loss: 0.000014\n",
      "Epoch 117/599\n",
      "----------\n",
      "train: loss: 0.000025\n",
      "LR 0.001\n",
      "val: loss: 0.000023\n",
      "Epoch 118/599\n",
      "----------\n",
      "train: loss: 0.000025\n",
      "LR 0.001\n",
      "val: loss: 0.000011\n",
      "saving best model\n",
      "Epoch 119/599\n",
      "----------\n",
      "train: loss: 0.000024\n",
      "LR 0.001\n",
      "val: loss: 0.000016\n",
      "Epoch 120/599\n",
      "----------\n",
      "train: loss: 0.000024\n",
      "LR 0.001\n",
      "val: loss: 0.000013\n",
      "Epoch 121/599\n",
      "----------\n",
      "train: loss: 0.000024\n",
      "LR 0.001\n",
      "val: loss: 0.000013\n",
      "Epoch 122/599\n",
      "----------\n",
      "train: loss: 0.000026\n",
      "LR 0.001\n",
      "val: loss: 0.000018\n",
      "Epoch 123/599\n",
      "----------\n",
      "train: loss: 0.000025\n",
      "LR 0.001\n",
      "val: loss: 0.000029\n",
      "Epoch 124/599\n",
      "----------\n",
      "train: loss: 0.000023\n",
      "LR 0.001\n",
      "val: loss: 0.000015\n",
      "Epoch 125/599\n",
      "----------\n",
      "train: loss: 0.000025\n",
      "LR 0.001\n",
      "val: loss: 0.000012\n",
      "Epoch 126/599\n",
      "----------\n",
      "train: loss: 0.000024\n",
      "LR 0.001\n",
      "val: loss: 0.000027\n",
      "Epoch 127/599\n",
      "----------\n",
      "train: loss: 0.000023\n",
      "LR 0.001\n",
      "val: loss: 0.000012\n",
      "Epoch 128/599\n",
      "----------\n",
      "train: loss: 0.000023\n",
      "LR 0.001\n",
      "val: loss: 0.000027\n",
      "Epoch 129/599\n",
      "----------\n",
      "train: loss: 0.000028\n",
      "LR 0.001\n",
      "val: loss: 0.000016\n",
      "Epoch 130/599\n",
      "----------\n",
      "train: loss: 0.000022\n",
      "LR 0.001\n",
      "val: loss: 0.000021\n",
      "Epoch 131/599\n",
      "----------\n",
      "train: loss: 0.000023\n",
      "LR 0.001\n",
      "val: loss: 0.000011\n",
      "saving best model\n",
      "Epoch 132/599\n",
      "----------\n",
      "train: loss: 0.000023\n",
      "LR 0.001\n",
      "val: loss: 0.000010\n",
      "saving best model\n",
      "Epoch 133/599\n",
      "----------\n",
      "train: loss: 0.000021\n",
      "LR 0.001\n",
      "val: loss: 0.000010\n",
      "Epoch 134/599\n",
      "----------\n",
      "train: loss: 0.000022\n",
      "LR 0.001\n",
      "val: loss: 0.000059\n",
      "Epoch 135/599\n",
      "----------\n",
      "train: loss: 0.000021\n",
      "LR 0.001\n",
      "val: loss: 0.000015\n",
      "Epoch 136/599\n",
      "----------\n",
      "train: loss: 0.000021\n",
      "LR 0.001\n",
      "val: loss: 0.000015\n",
      "Epoch 137/599\n",
      "----------\n",
      "train: loss: 0.000022\n",
      "LR 0.001\n",
      "val: loss: 0.000013\n",
      "Epoch 138/599\n",
      "----------\n",
      "train: loss: 0.000021\n",
      "LR 0.001\n",
      "val: loss: 0.000053\n",
      "Epoch 139/599\n",
      "----------\n",
      "train: loss: 0.000022\n",
      "LR 0.001\n",
      "val: loss: 0.000008\n",
      "saving best model\n",
      "Epoch 140/599\n",
      "----------\n",
      "train: loss: 0.000019\n",
      "LR 0.001\n",
      "val: loss: 0.000009\n",
      "Epoch 141/599\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "optimizer_ft = torch.optim.Adam(hybrid_L63.parameters(), lr=0.01)\n",
    "exp_lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer_ft, step_size=100, gamma=0.1)\n",
    "model_best_valid, model, loss_train, loss_val = train_L63(hybrid_L63, dataloaders, optimizer_ft, exp_lr_scheduler, device=params['device'], num_epochs=params['ntrain'], dt=params['dt_integration'], seq_size=params['seq_size'], grad_mode=params['grad_mode'])\n",
    "torch.save(model.state_dict(), params['output_folder'] + params['model_save_file_name'])"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-10-23T16:34:06.516305Z"
    }
   },
   "id": "600d820290ff1337",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "initial_condition_test = next(iter(dataloaders['val']))[0][0,:]\n",
    "simulation_hybrid = odeint(model_best_valid.dyn_net,initial_condition_test,np.arange(0,40+0.000001,GD.dt_integration),args=(False,))\n",
    "simulation_init_sys  = odeint(model_best_valid.dyn_net,initial_condition_test,np.arange(0,40+0.000001,GD.dt_integration),args=(False,))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "6e2363b31d5f112f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create 3D plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plotting the line in 3D\n",
    "ax.plot(dataset[:,0], dataset[:,1], dataset[:,2], label='True Attractor', alpha = 0.3)\n",
    "ax.plot(simulation_hybrid[:,0], simulation_hybrid[:,1], simulation_hybrid[:,2], label='Hybrid model, EGA Ensemble ')\n",
    "ax.plot(simulation_init_sys[:,0], simulation_init_sys[:,1], simulation_init_sys[:,2], label='Physical core')\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "a5a3d8934ee3ca2f",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "531a8dc588dac8d0",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
